h1. Fabric
{toc}

* [Fabric in One Page|10-fabric-in-one-page]
* [Platform Shape|20-platform-shape]
* [Governance First|30-governance-first]
* [Access & Query|40-access-and-query]
* [Ingest & Delivery|50-ingest-and-delivery]
* [Services & Execution|60-services-and-execution]
* [End-to-End Use Case|70-end-to-end-use-case]
* [Metadata & Configuration|80-meta-config]
* [Fabric Data Ponds|90-data-ponds]



----

h1. Fabric in One Page


h2. What Fabric is
Fabric is a unified data and service platform that provides a single governed interface for ingesting, accessing, and delivering data across multiple transports and runtimes.

It sits between clients and existing systems, allowing producers and services to remain unchanged while Fabric standardizes access, governance, and operational behavior.

h2. What is available
Fabric provides a cohesive set of capabilities that can be adopted incrementally:

* *Entitlements* — A single, centralized access model that governs datasets, streams, and services consistently across all transports.
* *Ingestion* — Governed ingestion and routing of data from multiple sources into managed datasets and services.
* *Datasets* — Unified, tabular access to datasets through a single interface, independent of where data is stored or executed.
* *Streaming* — Publish/subscribe access to streams across multiple transports with consistent identity, schema, and entitlement enforcement.
* *Services* — Governed exposure of services across multiple runtime protocols with access control applied at the platform boundary.
* *Materialized* — Declaratively defined derived datasets managed as first-class platform assets.
* *Storage* — Open data storage using standard formats, accessible outside Fabric without loss of governance.
* *APIs* — A single client API for accessing datasets and streams across all supported transports, available in Java and Python, designed for efficient, zero-copy data access.

These capabilities are exposed through a single platform interface so clients interact with Fabric rather than individual systems.

h2. Governance-first platform
Governance is applied at the Fabric boundary rather than embedded in individual transports or storage systems.

Access is evaluated using a single entitlement model that applies consistently across datasets, streams, and services. Requests are authorized once at the platform boundary and enforced uniformly, regardless of how data is accessed or delivered.

Fabric supports acting on behalf of an end user (OBO) so services and applications can safely propagate user identity across transports while preserving auditability and policy enforcement.

h2. Open, portable, and deployable anywhere
Fabric is built around open standards and formats so data and services are not locked into proprietary systems.

Storage and compute can be local to a team or domain, allowing groups to operate independent “data ponds” using their own infrastructure while still benefiting from centralized governance.  

At the same time, Fabric integrates cleanly with strategic, centralized data lake platforms, allowing datasets to live in or be sourced from those environments when shared, large-scale analytics or cross-domain access is required. Fabric provides a consistent, governed access layer across both models, enabling teams to choose the appropriate storage and execution footprint for each workload.

Fabric is developed in-house with minimal third-party runtime dependencies. This deliberate design choice keeps the platform lean, reduces external vendor risk, and minimizes exposure to third-party CVEs while maintaining full control over behavior and security posture.

Fabric runs in standard environments, from developer machines to server hosts and container platforms, enabling consistent behavior across development and production deployments.


----

h1. Platform Shape


h2. Deployment topologies
Fabric supports multiple deployment topologies while preserving a consistent client experience:

* Local development on individual workstations for rapid iteration and testing.
* Shared service deployments on Linux hosts or virtual machines for team and application use.
* Container-based deployments (e.g. OpenShift) using the same configuration-driven model.
* Identical client interaction model across all deployment environments.

h2. Storage placement
Fabric does not mandate a single centralized storage system:

* Local storage for development and domain-specific workloads.
* Shared storage such as SAN for performance-sensitive or managed environments.
* Object storage such as S3 for scalable and durable data persistence.
* Domain-owned storage (“data ponds”) where teams manage their own compute and storage.
* Fabric provides governed access rather than imposing a proprietary storage layer.

Fabric can also act as a governed access layer over existing large-scale data platforms, including shared data lakes and specialized analytics systems.

h2. Global and multi-environment deployments
Fabric supports operation across environments and locations:

* Environment metadata separates development, testing, and production deployments.
* Region metadata enables global and multi-site deployments.
* Consistent discovery and access model across environments.
* Centralized governance and access control even when data and services are distributed.

Clients do not need to change behavior based on environment or location.

h2. Client-side integration and consumption
Fabric integrates directly with how clients and analysts work:

* Single client API surface for accessing datasets and streams.
* Java and Python client support.
* Native fit for interactive workflows such as notebooks.
* Efficient consumption by analytics tools and libraries (e.g. dataframes and array-based workflows).
* Zero-copy data exchange where possible to minimize serialization and data movement.
* Governance, identity, and access control preserved end-to-end.

h2. Integration reach
Fabric is designed for incremental adoption rather than platform replacement:

* Acts as a governed access layer over existing services and messaging systems.
* Allows clients to adopt Fabric immediately as their primary interface.
* Existing service providers and data platforms can remain unchanged.
* Migration to native Fabric patterns is optional and can occur over time.

This model enables organizations to standardize access and governance without disrupting existing systems.


----

h1. Governance First


h2. A single governance model
Fabric applies governance as a first-class platform concern rather than an afterthought.

A single entitlement model is used consistently across:
* Datasets
* Streams
* Services
* Queries and APIs

The same access rules apply regardless of *how* something is accessed — whether through a query, a stream subscription, or a service call.

Fabric enforces governance at the platform boundary, removing the need for each transport, engine, or service to implement its own authorization logic.

h2. Entitlements foundation

Fabric’s entitlement model is inspired by *Zanzibar*, an authorization system designed and described by Google.

Zanzibar is not a third-party product or service. It is a *well-established architectural pattern* for large-scale, fine-grained authorization, based on representing access control as relationships between subjects and resources and evaluating those relationships dynamically at request time.

Google uses this model internally to enforce access control across many of its largest systems, including user data, services, and internal APIs, at global scale.

Fabric implements this model natively as part of the platform, rather than integrating an external authorization system. This allows Fabric to provide a consistent, explainable, and auditable entitlement model across datasets, streams, and services, while remaining fully self-contained and deployable in enterprise environments.

At a high level:
* Access is expressed as relationships between *subjects* and *resources*
* Relationships are evaluated dynamically at request time
* Authorization decisions are consistent, auditable, and explainable

For readers interested in the underlying design, Google’s public paper provides a concise overview of the Zanzibar model and its motivations:
* [Zanzibar: Google’s Consistent, Global Authorization System|https://research.google/pubs/pub48190/]

h2. Who can be entitled
Fabric supports entitlements for multiple actor and subject types:

* Users — individual human identities
* Groups — logical collections of users
* Applications — service and system identities

Access can be granted directly or inherited indirectly (for example, users gaining access via group membership).

h2. What can be governed
Fabric applies entitlements uniformly across platform resources, including:

* Datasets
** Table-level access
** Row-level access
** Column-level access
* Streams
** Publish and subscribe permissions
* Services
** Invocation and lifecycle control
* Queries and APIs
** Consistent enforcement regardless of access path

This unified approach allows organizations to define access once and apply it everywhere.

h2. On-Behalf-Of (OBO) and identity propagation
Fabric supports acting *On Behalf Of* (OBO) an end user across all access paths.

OBO allows a service or application to call Fabric while explicitly declaring:
* The *actor* making the request (the application)
* The *subject* on whose behalf the request is being made (the end user)

Fabric validates delegated identity using supported enterprise mechanisms, including:
* Validated SSO-based user identity
* Kerberos credential forwarding (double-hop) where available

This ensures that:
* End-user identity is propagated end-to-end
* Entitlements are evaluated against the correct user context
* Delegated access is auditable and explainable

Due to firm constraints, *SSO-to-Kerberos impersonation via S4U2 is not supported*. Fabric does not rely on S4U2 and instead validates and forwards already-authenticated credentials where applicable.

h2. Authentication integration
Fabric integrates directly with enterprise authentication mechanisms, including:

* Single Sign-On (SSO)
* Kerberos-based authentication and delegation

Authentication establishes identity, while entitlements determine *what that identity is allowed to do*. These concerns are deliberately separated to keep the model clear, auditable, and adaptable across environments.

h2. Audit, traceability, and compliance
Governance in Fabric is fully observable.

Fabric provides centralized, non-blocking audit capture for all governance-relevant activity, including:
* Metadata and configuration changes
* Dataset, stream, and service access decisions
* OBO usage and delegated access
* Runtime status and lifecycle events

Audit events are:
* Captured centrally by the platform
* Persisted in open, queryable formats (e.g. Parquet)
* Searchable and explorable through administrative tooling

This provides:
* End-to-end traceability of *who did what, when, and under which identity*
* Strong foundations for compliance, regulatory review, and incident investigation
* Confidence that governance decisions can be reconstructed and explained after the fact

h2. Simple entitlement examples

The following examples show how the same entitlement model governs *services*, *datasets*, and *combined service + data access with OBO*.

h3. Example: governed service lifecycle and invocation
{code:language=json}
{
  "payload": {
    "scopeType": "service",
    "scopeId": "service:oms/orders",
    "policies": [
      {
        "policyId": "oms.service.orders",
        "type": "SERVICE",
        "resourceId": "service:oms/orders",
        "bindings": [
          {
            "subject": "group:oms",
            "action": "service.start",
            "effect": "ALLOW"
          },
          {
            "subject": "group:desk1",
            "action": "service.invoke",
            "effect": "ALLOW"
          }
        ]
      }
    ]
  }
}
{code}

* OMS can start the *orders* service
* Desk1 can invoke the service
* The same policy applies regardless of gRPC, Arrow Flight, or other runtimes

h3. Example: governed dataset access with row-level filtering
{code:language=json}
{
  "payload": {
    "scopeType": "dataset",
    "scopeId": "dataset:oms/orders_hist",
    "policies": [
      {
        "policyId": "oms.dataset.orders_hist",
        "type": "DATASET",
        "resourceId": "dataset:oms/orders_hist",
        "bindings": [
          {
            "subject": "group:oms",
            "action": "data.write",
            "effect": "ALLOW"
          },
          {
            "subject": "group:desk1",
            "action": "data.read",
            "effect": "ALLOW",
            "obligations": {
              "rowFilters": [
                "desk = 'DESK1'"
              ]
            }
          }
        ]
      }
    ]
  }
}
{code}

* OMS owns and writes the dataset
* Desk1 can read the dataset
* Desk1 automatically receives only rows matching its entitlement filter

h3. Example: combined service + dataset policy with OBO context
This example illustrates how a service invocation and dataset access are evaluated *together* under OBO.

{code:language=json}
{
  "payload": {
    "policies": [
      {
        "policyId": "oms.service.orders",
        "type": "SERVICE",
        "resourceId": "service:oms/orders",
        "bindings": [
          {
            "subject": "group:desk1",
            "action": "service.invoke",
            "effect": "ALLOW"
          }
        ]
      },
      {
        "policyId": "oms.dataset.orders_hist",
        "type": "DATASET",
        "resourceId": "dataset:oms/orders_hist",
        "bindings": [
          {
            "subject": "group:desk1",
            "action": "data.read",
            "effect": "ALLOW",
            "obligations": {
              "rowFilters": [
                "desk = 'DESK1'"
              ]
            }
          }
        ]
      }
    ]
  }
}
{code}

In this flow:
* Desk1 invokes the *orders* service
* The service calls Fabric *on behalf of* the Desk1 user
* Fabric evaluates both service and dataset policies using the same OBO subject
* Row-level filtering is enforced automatically during query execution
* Audit records capture actor, subject, service, dataset, and decision context

h2. Pluggable sources of truth
Fabric is designed to support multiple entitlement sources of truth.

Entitlements can be derived from platform-managed configuration as well as integrated enterprise identity and authorization systems.

Regardless of where entitlements originate, Fabric remains the enforcement point, ensuring consistent behavior across all access paths.

h2. Security & Governance Summary

Fabric’s security and governance model forms a single, end-to-end control loop:

*Identity → OBO → Entitlements → Audit → Explainability*

*Identity*  
Every request entering Fabric is associated with a clear, authenticated identity.

*On-Behalf-Of (OBO)*  
Delegated access explicitly captures both actor and subject.

*Entitlements*  
Policies are evaluated once and applied uniformly everywhere.

*Audit*  
All governance-relevant actions are captured centrally.

*Explainability*  
Every allow or deny decision can be reconstructed and explained.

Together, these components ensure that Fabric delivers strong security and governance without sacrificing developer productivity, performance, or architectural flexibility.


----

h1. Access & Query


h2. Unified access model
Fabric provides a single, consistent way for clients to access data.

Clients interact with Fabric through a unified tabular and streaming interface, independent of where data originates, how it is stored, or which runtime executes the request. Dataset and stream identity is canonical, allowing clients to reason about data logically rather than embedding transport or infrastructure details.

The same access model applies across datasets, streams, services, and derived views.

h2. Datasets (pull / SQL)
Fabric exposes datasets through a unified SQL-based query interface.

* Clients query datasets using standard SQL.
* Projection and predicate push-down ensure clients request only the columns and rows they need.
* Datasets can be joined using standard SQL, including joins across independently owned domains.
* Authorization and entitlements are enforced consistently for every query.
* Acting On Behalf Of (OBO) is supported so queries can be evaluated using end-user identity.

From a client perspective, querying a dataset is a stable, uniform operation regardless of where the data lives.

h3. Python example (dataset query)
A typical quant workflow is: open a session, run SQL, and consume results in familiar analytical tools.

{code:language=python}
import fabric
import polars as pl

session = fabric.Session().environment("DEV").group("lab").connect()
tabular = session.tabular()

result = tabular.query("""
    SELECT order_id, price, quantity
    FROM orders
    WHERE trade_date >= DATE '2026-01-01'
""")

df = pl.from_arrow(result.to_arrow())
df = df.with_columns((pl.col("price") * pl.col("quantity")).alias("notional"))

print(df.head())
{code}

h2. Result formats and client consumption
Fabric allows clients to choose how query and stream results are materialized.

The same logical query can be consumed in different formats depending on the client’s needs:

* Columnar formats (analytics and compute)
** Arrow (columnar, zero-copy where possible)
** NumPy-compatible arrays
** Polars DataFrames
** Parquet files
* Row-based formats (applications and integration)
** JSON
** CSV
** FIX / domain-specific encodings (e.g. nvFIX)
** JDBC-style ResultSet (Java)

This flexibility allows:
* Quants to work directly with columnar, vectorized data
* Applications to consume row-oriented results
* Integration with existing tools without rewriting client logic

Format selection does not change governance behavior — entitlements and policies are enforced identically regardless of how results are consumed.

h2. Streams (subscribe)
Fabric also exposes streams as first-class platform resources.

* Streams can be subscribed to using a unified API, regardless of the underlying messaging system.
* Stream schemas and identity are governed centrally.
* Entitlements are enforced on publish and subscribe operations.
* Subscriptions can be evaluated using application identity or OBO user identity.

This allows streaming data to be consumed with the same governance and identity model used for datasets.

h3. Python example (stream subscribe)
{code:language=python}
import fabric
import polars as pl

session = fabric.Session().environment("DEV").group("lab").connect()
streams = session.streams()

subscription = streams.subscribe("orders_stream")

for batch in subscription:
    # batch is a tabular record batch
    df = pl.from_arrow(batch)
    process(df)
{code}

h2. Materialized views and enrichment
Fabric supports materialized datasets defined using SQL.

* Materialized views can join streaming data with static reference datasets.
* Views are treated as first-class governed assets.
* Entitlements apply to the view in the same way as base datasets.
* Clients query materialized views using the same SQL interface as any other dataset.

This enables common enrichment patterns such as:
* Streaming orders joined with static reference data
* Normalization of multiple feeds into a single analytical view
* Precomputed datasets for repeated access patterns

h2. Existing AMPS and gRPC integration
Fabric can adapt existing AMPS topics and gRPC services into the unified access model.

* AMPS topics can be exposed as datasets or streams that support SQL queries.
* Clients can issue full SQL queries over AMPS-backed data with entitlements enforced per user or group.
* This extends governance beyond topic-level service identities to fine-grained dataset and query access.
* Existing gRPC services can be adapted so Fabric becomes the governed entry point without requiring provider changes.

For organizations with large existing AMPS and gRPC estates, this allows immediate client-side benefits — unified access, SQL, and entitlements — without disrupting current producers or services.

h2. Client-facing APIs
Fabric exposes a single client API surface for accessing datasets and streams.

* Java and Python clients provide access to the same logical capabilities.
* Clients select the result format that best fits their workflow.
* Execution, transport, and storage details are abstracted away from clients.
* Governance, entitlements, and OBO enforcement are applied consistently across all access paths.

This enables quants, applications, and services to interact with data using one model, regardless of source, delivery mechanism, or consumption style.


----

h1. Ingest & Delivery


h2. Unified ingest and delivery model
Fabric provides a governed, platform-level model for getting data into the system and delivering it to consumers.

Ingestion and streaming are treated as first-class platform capabilities, not bespoke pipelines. Fabric standardizes identity, schema, and governance at the boundary, while allowing producers and delivery mechanisms to evolve independently.

This ensures that data is governed consistently from the moment it enters the platform through every downstream access path.

h2. Feeds (ingest)
Feeds provide a governed ingestion boundary for bringing data into Fabric.

* Feeds define *how* data enters the platform, independent of where it comes from.
* Dataset identity is assigned and managed centrally.
* Schema is validated and enforced at ingest time.
* Producers are not required to adopt a specific runtime or storage system.
* Open formats can be used at the ingest edge to keep data portable.

Feeds allow upstream producers to remain unchanged while Fabric standardizes how data is identified, validated, and governed.

h3. Logical feed patterns
The following table illustrates common *logical* feed patterns supported by Fabric:

|| Source || Target || Notes ||
| Files | Datasets | Batch or incremental ingest from filesystem-based sources |
| AMPS | Datasets | Ingest from existing AMPS topics into governed datasets |
| Kafka | Datasets | Stream-to-dataset ingestion with schema enforcement |
| Aeron | Datasets | High-throughput ingest into managed datasets |
| gRPC | Datasets | Service-driven ingestion without exposing internal schemas |
| Files | Streams | File-backed publish into streaming topics |
| Datasets | Streams | Dataset-backed streaming delivery |

These patterns describe *how* data flows conceptually; the physical implementations are shown below.

h3. Physical feed source and target matrix
The table below summarizes the *physical* source and target combinations supported by the Fabric Feed API today.

This reflects concrete implementations rather than conceptual patterns.

|| Source \ Target || Local filesystem || S3 (Parquet) || HDFS (Parquet) || In-memory dataset || DuckDB dataset || Stream (AMPS / Kafka / Aeron) ||
| Local filesystem | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| S3 (Parquet) | ✓ | — | ✓ | ✓ | ✓ | ✓ |
| HDFS (Parquet) | ✓ | ✓ | — | ✓ | ✓ | ✓ |
| AMPS (JSON / txn log) | — | ✓ | ✓ | ✓ | ✓ | ✓ |
| Kafka | — | ✓ | ✓ | ✓ | ✓ | ✓ |
| Aeron | — | ✓ | ✓ | ✓ | ✓ | ✓ |
| gRPC service | — | ✓ | ✓ | ✓ | ✓ | ✓ |
| Fabric dataset | — | — | — | — | — | ✓ |

*Legend:*  
✓ = supported directly by Fabric feeds  
— = not applicable or not meaningful

*Notes*
* Local filesystem includes local disk and SAN-mounted paths.
* S3 and HDFS targets persist data in open formats (typically Parquet).
* In-memory datasets are transient and commonly used for staging or transformation.
* DuckDB datasets are managed, queryable datasets within Fabric.
* Stream targets include AMPS, Kafka, and Aeron depending on deployment.
* gRPC sources rely on adapters that translate service responses into tabular records.
* Some combinations depend on connector configuration and credentials, but do not require custom code.

h3. Python example (dataset ingest)
A simple example of ingesting data into a dataset using a feed:

{code:language=python}
session = fabric.Session().environment("DEV").group("lab").connect()
feeds = session.feeds()

feeds.publish(
    dataset="orders",
    records=[
        {"order_id": 1, "price": 100.0, "quantity": 10},
        {"order_id": 2, "price": 250.0, "quantity": 5}
    ]
)
{code}

The dataset identity, schema validation, and entitlements are handled by Fabric at the platform boundary.

h2. Streaming (publish / subscribe)
Fabric exposes streaming as a unified publish/subscribe model.

* Streams are first-class, governed resources.
* Publish and subscribe operations are consistent across transports.
* Stream schemas and metadata are managed centrally.
* Entitlements are enforced for both producers and consumers.

This allows streaming systems to be integrated into Fabric without leaking transport-specific semantics to clients.

h3. Python example (stream publish)
{code:language=python}
session = fabric.Session().environment("DEV").group("lab").connect()
streams = session.streams()

streams.publish(
    stream="orders_stream",
    record={"order_id": 3, "price": 500.0, "quantity": 2}
)
{code}

Producers interact with Fabric, not the underlying messaging system, while governance is applied uniformly.

h2. Delivery to downstream consumers
Fabric supports delivering ingested data through multiple access paths:

* Pull-based access via SQL queries on datasets.
* Push-based access via stream subscriptions.
* Derived delivery through materialized datasets.
* Exposure through governed services.

All delivery paths use the same identity, schema, and entitlement model.

h2. Governance alignment
Ingest and delivery are governed in the same way as read access.

* Entitlements are enforced at the platform boundary for ingest, publish, and subscribe operations.
* Acting On Behalf Of (OBO) is supported so writes and publishes can be attributed to end-user identity.
* Schema violations and unauthorized access are rejected early and consistently.

This ensures that governance is applied *from the moment data enters the platform*, not retrofitted later.

h2. Integration with existing systems
Fabric is designed to integrate with existing ingestion and streaming systems.

* Existing AMPS topics can be ingested and exposed as governed datasets or streams.
* Kafka and Aeron can participate in the same ingest and delivery model.
* Existing producers do not need to change to participate in Fabric.
* Clients gain immediate benefits: unified identity, schema enforcement, and entitlements.

This enables organizations to standardize ingest and delivery without disrupting current producers or pipelines.


----

h1. Services & Execution


h2. Governed service execution
Fabric provides a unified, governed execution model for exposing services to clients.

Services are registered and exposed through Fabric, which becomes the consistent entry point for discovery, routing, security, and entitlement enforcement. Service providers can use different runtime technologies, while clients interact through a single, uniform access model.

Fabric centralizes access control, identity propagation, and request routing at the platform boundary, rather than embedding these concerns inside each service implementation.

h2. What Fabric takes care of
Service providers and clients focus on business logic. Fabric provides the cross-cutting platform capabilities once, consistently, across all supported runtimes.

Fabric handles:
* Service discovery and routing (clients target a logical service name, not hosts)
* TLS, Kerberos, SSO, and OBO identity propagation
* Entitlements enforcement and auditability at the platform boundary
* Compatibility and upgrade safety (service evolution without breaking clients)
* Centralized interceptors (logging, tracing, standard headers, policy context)
* Backpressure, retries, and failure handling at the platform boundary (consistent behavior)
* Metrics and operational visibility (usage, denies, errors, status)

This reduces duplicated boilerplate in every service and makes service integration predictable for both producers and consumers.

h2. Invocation patterns supported
Fabric supports the full set of common service interaction models:

* Request/response (unary calls)
* Server-side streaming (streaming responses)
* Client-side streaming (streaming requests)
* Bidirectional streaming (streaming requests and responses)

These patterns are exposed through the same governed entry point, with consistent identity, entitlements, and auditing.

h2. AMPS services
Fabric supports AMPS-based services as first-class execution runtimes.

* Publish/subscribe services for event-driven and streaming use cases.
* Private inbox (request/reply) services for point-to-point interactions.
* Centralized entitlements enforced per user, group, or application.
* Consistent identity propagation across AMPS interactions.

This allows organizations with large existing AMPS estates to expose services through Fabric without rewriting producers or consumers.

h3. Python example (AMPS publish)
{code:language=python}
streams = session.streams()

streams.publish(
    stream="pricing_updates",
    record={"symbol": "AAPL", "price": 187.42}
)
{code}

h2. gRPC services
Fabric exposes gRPC services through a governed execution layer.

* gRPC services are registered with Fabric and exposed through a common access boundary.
* Client calls are authorized and audited consistently.
* Services can scale horizontally without client-side changes.
* Clients are automatically routed to available service instances.

Existing gRPC services can be adapted so Fabric becomes the entry point, without requiring changes to service implementations.

h3. Java example (gRPC service provider — minimal)
Assume *session* is already created and started.

{code:language=java}
ServiceRunOptions options =
    ServiceRunOptions.builder()
        .withRuntimeType("grpc")
        .withInstanceName("service-echo")
        .withBindHost("0.0.0.0")
        .withBindPort(50051)
        .withAdvertiseHost("127.0.0.1")
        .withAdvertisePort(50051)
        .withTls(
            Path.of("vault/tls/demo-service.pem"),
            Path.of("vault/tls/demo-service.key"))
        .build();

try (ServiceHandle service =
    session.services().start("echo", options, new SimpleEchoService())) {
  service.blockUntilShutdown();
}
{code}

h3. Java example (gRPC client — minimal)
Assume *session* is already created and started.

{code:language=java}
ServiceRunOptions options =
    ServiceRunOptions.builder()
        .withRuntimeType("grpc")
        .withTrustCert(Path.of("vault/tls/demo-ca.pem"))
        .build();

try (ServiceClient client = session.services().client("echo", options)) {
  demo.echo.EchoGrpc.EchoBlockingStub stub =
      client.unwrap(demo.echo.EchoGrpc.EchoBlockingStub.class);

  var reply =
      stub.say(demo.echo.EchoRequest.newBuilder().setMsg("hello").build());

  System.out.println(
      "reply msg='" + reply.getMsg() + "' instance='" + reply.getInstance() + "' group='" + reply.getGroup() + "'");
}
{code}

h2. Arrow Flight services
Fabric supports Arrow Flight as a service execution runtime for high-performance data exchange.

* Services expose Arrow-native interfaces for tabular data transfer.
* Clients interact with Flight services through Fabric’s governed entry point.
* Entitlements and identity are enforced consistently with other runtimes.
* Flight services can be scaled independently of clients.

This model is well suited for analytical and compute-heavy services that benefit from columnar data transfer.

h2. Arrow Flight SQL services
Fabric also supports Arrow Flight SQL as a governed execution runtime.

* SQL services are exposed through a standard Flight SQL interface.
* Clients issue SQL queries without needing to know where the service runs.
* Results are delivered in tabular form with consistent entitlement enforcement.
* Flight SQL services can back datasets, materialized views, or analytical services.

This allows SQL-based services to participate fully in the same governance and routing model as other Fabric services.

h2. Routing and scalability
Fabric manages service discovery and client-side routing.

* Clients automatically route to available service instances.
* Multiple routing strategies can be applied without client changes.
* Scaling services up or down does not require client reconfiguration.
* Failover and instance availability are handled by the platform.

From a client perspective, a service is a logical endpoint rather than a specific host.

h2. Security and identity at execution time
Fabric applies a consistent security and identity model across all service executions.

* Secure transport using TLS.
* Integration with Kerberos and enterprise authentication mechanisms.
* Support for Acting On Behalf Of (OBO) in service calls.
* Clear separation between service identity and end-user identity.

Security and entitlement enforcement occur at execution time, uniformly across all runtimes.

h2. Deployment environments
Fabric services can be deployed across multiple environments.

* Local development for rapid iteration.
* Linux hosts or virtual machines for shared services.
* Container platforms such as OpenShift for managed deployments.

Deployment behavior is configuration-driven, allowing the same service execution model to be used from development through production without changing client behavior.


----

h1. End-to-End Use Case


h2. Scenario overview
This example shows how Fabric connects ingestion, storage, query, and entitlements into a single governed workflow using a realistic *orders history* use case.

The key point of this example is that *the same dataset identity, policies, and client code can be used with different architectures*, depending on how teams choose to deploy Fabric and how quickly upstream providers adopt it.

h3. Teams and responsibilities
* *OMS* (producer team)
** Owns the *orders_hist* dataset
** Publishes historical orders data (either via Fabric feeds or via an existing service)
** Defines entitlements and row-level policies for consumers
* *Desk1* (consumer team)
** Queries *orders_hist* using the Java Tabular API
* *Desk2* (consumer team)
** Queries *orders_hist* using the Python Tabular API
** Sees a filtered view of the data via row-level entitlements

All teams interact through Fabric’s canonical dataset identity: *orders_hist*.

h2. Step 1: OMS defines and ingests the dataset
The OMS team defines a governed dataset and begins ingesting orders history data.

h3. Define the dataset and schema (Feed API)
{code:language=java}
// OMS team — define a governed dataset with an inline schema
feeds.getOrCreate(
    FeedModel.builder()
        .withName("orders_hist")
        .withTarget(target -> target
            .withParquet(parquet -> parquet
                .withDirectory("s3://fabric-data/orders_hist")
                .withPartition(List.of("created_date"))
            )
        )
        .withSchema(
            FeedSchema.builder()
                .addField(field("order_id", FeedSchema.Type.STRING, false))
                .addField(field("desk", FeedSchema.Type.STRING, false))
                .addField(field("price", FeedSchema.Type.FLOAT64, false))
                .addField(field("created_date", FeedSchema.Type.DATE, false))
                .build()
        )
        .build()
);
{code}

h3. Ingest orders data
{code:language=java}
// OMS team — publish order records
feeds.ingest("orders_hist", List.of(
    Map.of(
        "order_id", "ORD-1001",
        "desk", "DESK1",
        "price", 125.50,
        "created_date", Date.valueOf(LocalDate.now())
    ),
    Map.of(
        "order_id", "ORD-1002",
        "desk", "DESK2",
        "price", 250.75,
        "created_date", Date.valueOf(LocalDate.now())
    )
));
{code}

Fabric validates schema, applies partitioning, and stores the data in governed Parquet.

{code}
orders_hist/
├── created_date=2026-01-30/
│   ├── part-00000-3f2a.parquet
│   ├── part-00001-3f2a.parquet
│   └── part-00002-3f2a.parquet
├── created_date=2026-01-29/
│   ├── part-00000-9c81.parquet
│   └── part-00001-9c81.parquet
├── created_date=2026-01-28/
│   ├── part-00000-a41e.parquet
│   ├── part-00001-a41e.parquet
│   ├── part-00002-a41e.parquet
│   └── part-00003-a41e.parquet
└── _metadata
{code}

h2. Step 2: OMS defines entitlements and row-level policy
OMS defines dataset access for both desks, and a row-level filter for Desk2.

h3. Dataset read access
{code:language=json}
{
  "resource": "dataset:orders_hist",
  "permissions": [
    { "role": "reader", "subject": "group:desk1" },
    { "role": "reader", "subject": "group:desk2" }
  ]
}
{code}

h3. Row-level entitlement for Desk2
Desk2 can only see orders where the *desk* field equals *DESK2*.

{code:language=json}
{
  "resource": "dataset:orders_hist",
  "subject": "group:desk2",
  "row_filter": "desk = 'DESK2'"
}
{code}

This policy is enforced automatically for every access path.

h2. Architecture #1 — Service-based (Arrow Flight SQL)
In this architecture, OMS (or a platform team) runs scalable Arrow Flight SQL services on their own infrastructure.

* Data is written locally or to shared storage.
* Arrow Flight SQL services query the data directly.
* Services scale horizontally on VMs or OpenShift.
* Clients route automatically to available service instances.
* Fabric enforces entitlements and OBO inside both client and service runtimes.

This model fits teams that already operate services and want full control over runtime placement.

h3. Desk1 queries from Java
{code:language=java}
// Desk1 — Java client
var session = Fabric.Session.builder()
    .environment("DEV")
    .group("desk1")
    .build();

var tabular = session.tabular();

var result = tabular.query("""
    SELECT order_id, desk, price
    FROM orders_hist
    ORDER BY created_date DESC
""");

System.out.println(result.getRowCount());
{code}

h3. Desk2 queries from Python (automatically filtered)
{code:language=python}
import fabric
import polars as pl

session = fabric.Session().environment("DEV").group("desk2").connect()
tabular = session.tabular()

result = tabular.query("""
    SELECT order_id, desk, price
    FROM orders_hist
    ORDER BY created_date DESC
""")

df = pl.from_arrow(result.to_arrow())
print(df)
{code}

Desk2 automatically receives only rows where *desk = 'DESK2'*.

h2. Architecture #2 — Service-free (direct storage access)
In this architecture, no Arrow Flight SQL services are required.

* OMS writes partitioned Parquet directly to S3.
* Clients query *orders_hist* directly through Fabric.
* Fabric handles metadata resolution, push-down, and entitlements inline.
* No long-running service layer is required for reads or writes.

This model is especially attractive for:
* Analytical workloads
* Batch or ad-hoc querying
* Teams that prefer minimal operational footprint

The *same dataset identity, entitlements, and client code* are used as in the service-based architecture.

h3. Desk clients (Java and Python)
The client code remains unchanged from Architecture #1.

Fabric resolves the dataset location and enforces policies inline, allowing clients to query directly against storage without a service proxy.

h2. Architecture #3 — Adapter-based (existing OMS gRPC service)
In this architecture, OMS has not adopted Fabric yet and continues to expose orders history through an existing gRPC service.

Fabric still enables a unified client experience by using a Tabular gRPC adapter that can be embedded in the client runtime:

* OMS runs its existing gRPC service unchanged.
* Fabric clients continue to use the Tabular API.
* The Tabular gRPC adapter translates Tabular requests into OMS gRPC calls.
* Responses are mapped into a tabular/columnar form (e.g. Arrow) for efficient client consumption.
* Entitlements and row-level policies are still applied consistently at the Fabric boundary.

This provides immediate value to clients without forcing OMS to migrate upfront — while naturally creating demand for a future native Fabric service when the team is ready.

h3. Desk2 queries via the gRPC adapter (Python)
{code:language=python}
import fabric
import polars as pl

session = fabric.Session().environment("DEV").group("desk2").connect()

# Tabular API remains the same; routing points to a gRPC adapter behind the scenes
tabular = session.tabular()

result = tabular.query("""
    SELECT order_id, desk, price
    FROM orders_hist
    ORDER BY created_date DESC
""")

df = pl.from_arrow(result.to_arrow())
print(df)
{code}

From the client perspective:
* The API is unchanged.
* The dataset identity is unchanged.
* The row-level policy for Desk2 is still enforced.

Only the backend integration path differs.

h2. What this demonstrates
This single use case demonstrates Fabric’s core differentiators:

* The same dataset identity and policies support multiple deployment architectures.
* Fabric is an embedded runtime, not a central data service.
* Service-based, service-free, and adapter-based models can coexist.
* Clients use one API regardless of architecture.
* Entitlements and row-level policies are enforced consistently across access paths.
* Teams remain decoupled:
** producers publish without coordinating with consumers
** consumers query without knowing storage or service topology
** governance is defined once and enforced everywhere


----

h1. Metadata, Schemas, and Configuration


h2. A governed configuration platform
Fabric provides a first-class metadata and configuration platform for managing datasets, streams, services, and schemas.

Configuration is treated as data:
* Versioned
* Auditable
* Time-aware
* Governed by entitlements

Rather than embedding configuration in code or distributing it across systems, Fabric centralizes metadata and configuration in a controlled platform layer.

h2. What is managed
Fabric manages metadata and configuration for core platform resources, including:

* Datasets and dataset schemas
* Streams and stream metadata
* Services and service endpoints
* Feeds and ingest definitions
* Materialized datasets and derived views
* Environment- and region-specific settings

All metadata is identified canonically and referenced consistently across the platform.

h2. Layered and composable configuration
Fabric supports layered configuration models.

* Base configuration defines default behavior.
* Overlay layers can refine or override specific fields.
* Layers can be composed to produce an effective configuration.

This allows teams to:
* Share common defaults
* Customize behavior per environment or deployment
* Avoid duplication across configurations

h2. Effective time and safe rollout
Configuration changes in Fabric are time-aware.

* Changes can be defined with an effective timestamp.
* Future-dated configuration enables controlled rollout.
* Historical configurations remain queryable.

This allows configuration changes to be introduced safely without requiring coordinated restarts or manual cutovers.

h2. Auditability and rollback
All metadata changes are auditable.

* Every change is recorded with author, timestamp, and reason.
* Previous versions remain accessible.
* Rollback is explicit and controlled.

This enables teams to:
* Understand how a system reached its current state
* Revert changes safely
* Satisfy audit and compliance requirements

h2. Schema management and evolution
Fabric manages schemas as first-class metadata.

* Schemas are versioned and validated.
* Compatibility rules can be enforced on change.
* Schema evolution is governed, not ad hoc.

Schemas are used consistently across ingest, query, streaming, and service interfaces.

h2. Client APIs, subscriptions, and code generation
Metadata and configuration are accessible through client APIs and can be consumed dynamically.

* Applications can query effective configuration programmatically.
* Clients can subscribe to configuration updates and react to changes.
* Configuration is resolved centrally, not reimplemented in clients.
* Strongly-typed models can be generated from schema definitions.

This allows applications to treat configuration as a live, typed input rather than static files or environment variables.

h2. Example: subscribing to dataset configuration
A client can subscribe to configuration updates using a strongly-typed POJO.

h3. Configuration POJO (example)
{code:language=java}
public final class MoviesDatasetConfig {
  public String dataset;
  public String storageLocation;
  public String format;
}
{code}

h3. Subscribe and react to config changes
{code:language=java}
// Client application — subscribe to dataset metadata
Fabric.Session session = Fabric.newSession()
    .withGroup("media")
    .withName("movies-app")
    .build();

session.start();

session.meta().subscribe(
    "dataset:movies",
    MoviesDatasetConfig.class,
    config -> {
      System.out.println("Dataset: " + config.dataset);
      System.out.println("Format: " + config.format);
      System.out.println("Storage: " + config.storageLocation);
    }
);
{code}

When configuration changes become effective, the callback is invoked automatically with the resolved configuration.

h3. Example logged output
{code}
Dataset: movies
Format: parquet
Storage: s3://prod-media-datasets/movies
{code}

This allows applications to:
* React to configuration changes without restart
* Maintain a single source of truth
* Safely evolve behavior over time

h2. Example: dataset metadata (movies)
The following simplified example shows how a dataset might be defined:

{code:language=json}
{
  "dataset": "movies",
  "owner": "group:media",
  "schema": "movies_v1",
  "storage": {
    "format": "parquet",
    "location": "s3://media-datasets/movies"
  },
  "governance": {
    "readers": ["group:analysts"],
    "writers": ["app:movie-ingest"]
  }
}
{code}

This metadata defines:
* A canonical dataset identity
* Ownership and governance
* Storage location and format
* A schema reference used across the platform

h2. Example: layered configuration override
A production overlay might refine behavior without redefining the base:

{code:language=json}
{
  "dataset": "movies",
  "layer": "prod",
  "overrides": {
    "storage": {
      "location": "s3://prod-media-datasets/movies"
    }
  },
  "effective_from": "2026-03-01T00:00:00Z"
}
{code}

At runtime, Fabric resolves the effective configuration by combining base and overlay layers.

h2. Example: schema definition (movies)
Schemas are defined explicitly and versioned:

{code:language=json}
{
  "schema": "movies_v1",
  "fields": [
    { "name": "movie_id", "type": "string" },
    { "name": "title", "type": "string" },
    { "name": "release_year", "type": "int" },
    { "name": "genre", "type": "string" },
    { "name": "rating", "type": "double" }
  ]
}
{code}

Schema compatibility rules ensure safe evolution as datasets change.

h2. Why this matters
Fabric’s metadata and configuration model enables:

* Safe, audited change management
* Clear separation between code and configuration
* Live, reactive configuration for applications
* Consistent behavior across environments
* Strong governance without sacrificing flexibility

For organizations managing complex data and service estates, this provides a controlled foundation for scaling safely.


----

h1. Fabric Data Ponds


h2. Rethinking the default analytics footprint
Centralized enterprise data platforms were designed to solve problems at very large scale:
* Multi-petabyte storage
* Shared distributed compute
* Many teams querying the same raw data

These platforms remain essential for certain classes of workloads.  
However, in practice, many teams do not require these characteristics for their day-to-day analytics and operational workflows.

Instead, they often encounter:
* High infrastructure and operational cost
* Long onboarding and change cycles
* Multi-tenant contention and unpredictable performance
* Governance spread across multiple engines and tools

Fabric introduces the concept of *data ponds* — a complementary model optimized for domain-owned, high-velocity, and low-friction analytics.

h2. The small data movement
Industry experience increasingly shows that a large percentage of analytical workloads are:
* Domain-owned
* Bounded in size
* Accessed by a small number of teams
* Sensitive to latency and iteration speed

This shift is often referred to as the *Small Data* or *Embedded Analytics* movement.  
Modern columnar SQL engines such as DuckDB demonstrate that local or domain-owned data can deliver excellent analytical performance without requiring shared distributed compute.

Fabric adopts this model and extends it with identity, entitlements, and auditability suitable for enterprise environments.

For authoritative discussion of this approach, see:
* [DuckDB – Why DuckDB|https://duckdb.org/why_duckdb]
* [MotherDuck – Small Data Is the New Big Data|https://motherduck.com/blog/small-data-is-the-new-big-data/]

h2. Domain-owned data ponds
Fabric enables teams to operate *data ponds* alongside existing centralized platforms.

A Fabric data pond is:
* Owned by a single team or domain
* Stored on local disk, SAN, or object storage (e.g. S3)
* Governed centrally by Fabric
* Queried using standard SQL

Teams choose storage based on their needs:
* Local disk for development and low-latency workloads
* SAN for high-performance shared environments
* S3 for durable, low-cost storage

Fabric does not require a proprietary filesystem or a shared compute cluster.

h2. Governance without centralization
One of the primary reasons organizations adopt centralized data platforms is governance.

Fabric decouples governance from storage and compute:
* Entitlements are enforced at the access boundary
* The same policies apply across datasets, streams, and services
* Row- and column-level controls are supported
* On-Behalf-Of (OBO) ensures correct end-user identity propagation

This allows teams to operate independently while still meeting enterprise governance and audit requirements.

h2. SQL capability depth within data ponds
Fabric data ponds are not a “reduced SQL” experience.

Fabric’s SQL runtime supports advanced analytical patterns commonly used in quant, trading, and time-series workflows — without requiring a centralized data lake.

h3. Window functions
{code:language=sql}
SELECT
  symbol,
  trade_time,
  price,
  AVG(price) OVER (
    PARTITION BY symbol
    ORDER BY trade_time
    ROWS BETWEEN 10 PRECEDING AND CURRENT ROW
  ) AS rolling_avg_price
FROM trades;
{code}

h3. As-of joins (temporal joins)
{code:language=sql}
SELECT
  t.trade_id,
  t.trade_time,
  t.price,
  q.bid,
  q.ask
FROM trades t
ASOF JOIN quotes q
  ON t.symbol = q.symbol
 AND t.trade_time >= q.quote_time;
{code}

h3. SQL macros (reusable logic)
{code:language=sql}
CREATE MACRO notional(price, quantity) AS (
  price * quantity
);
{code}

{code:language=sql}
SELECT
  order_id,
  notional(price, quantity) AS notional
FROM orders;
{code}

h3. Pivot and unpivot
{code:language=sql}
SELECT *
FROM orders
PIVOT (
  SUM(quantity)
  FOR side IN ('BUY', 'SELL')
);
{code}

h2. Moving data between data ponds and centralized platforms
Fabric data ponds are not an isolated endpoint.  
They are designed to *interoperate* with centralized platforms using the same Feed API that governs all ingestion and delivery.

This enables two important workflows:

h3. Developing locally, scaling centrally
Teams can use Fabric data ponds for fast, iterative development and analytics, then move the resulting datasets into centralized platforms when scale or cross-domain access is required.

Typical flow:
* Ingest data into a Fabric data pond (local disk or S3).
* Develop and validate analytics using fast, local execution.
* Apply governance, schema, and entitlements early.
* Use a Fabric feed to export the dataset into a centralized platform (e.g. HDFS / data lake).

This supports a *develop locally, scale centrally* workflow without rewriting pipelines.

h3. Pulling data from centralized platforms into data ponds
Fabric can also ingest data from centralized platforms into data ponds for local or domain-specific analytics.

Typical flow:
* Ingest Parquet data from HDFS or S3-backed data lakes using Fabric feeds.
* Materialize a governed dataset in a Fabric data pond.
* Run low-latency, exploratory, or operational analytics locally.
* Avoid shared compute queues and multi-tenant contention.

This allows teams to work productively on subsets of large datasets without impacting shared infrastructure.

h3. Bidirectional movement using the same abstraction
Both directions use the same Fabric Feed API:
* Source and target are declared explicitly.
* Schema is validated at the boundary.
* Entitlements are enforced consistently.
* Storage format remains open (Parquet).

Fabric acts as the *governed bridge* between domain-owned data ponds and centralized platforms.

h2. S3-backed data ponds vs HDFS-centric platforms
|| Dimension || HDFS-centric platforms || S3-backed Fabric data ponds ||
| Small files | Actively discouraged | Natively supported |
| Operational overhead | Requires cluster management | Managed service |
| Onboarding friction | High | Low |
| Multi-tenant contention | Common | Isolated by bucket/prefix |
| File lifecycle | Manual | Native lifecycle policies |
| Cost model | Fixed cluster cost | Pay for storage used |
| Failure impact | Cluster-wide | Scoped |
| Access patterns | Batch-oriented | Batch + interactive |
| Governance | Often fragmented | Centralized in Fabric |
| Suitability for event/JSON data | Poor | Excellent |

h2. No shared compute requirement
Fabric data ponds do not require a centralized, multi-tenant compute layer.

Query execution can occur:
* Directly against domain-owned storage
* Inside team-operated services (e.g. Arrow Flight SQL)
* Via embedded execution invoked by clients

This eliminates:
* Queueing delays
* Resource contention
* Cross-team performance interference

Teams scale their own workloads independently.

h2. Fabric data ponds and centralized platforms
Fabric data ponds are not intended to replace centralized platforms universally.

Centralized platforms remain appropriate when:
* Many teams require access to the same raw data at very large scale
* Workloads require distributed joins across multiple domains
* Batch processing spans very large datasets

Fabric integrates with these systems and can act as a governed access layer when needed.

h2. When data ponds are the right choice
Fabric data ponds are well suited when:
* Data is owned and primarily used by a single team
* Workloads fit comfortably on local disk or object storage
* Low latency and iteration speed matter
* Governance must be enforced without heavy infrastructure

In these cases, Fabric allows teams to move faster with lower cost and fewer dependencies.

h2. Open formats and zero lock-in
Fabric relies exclusively on open standards:
* Parquet
* Arrow
* SQL

Data stored in Fabric-managed data ponds:
* Can be accessed outside Fabric using standard tools
* Is not locked into proprietary engines
* Can be migrated or integrated easily

This preserves long-term flexibility while still benefiting from a governed platform.

h2. Practical outcome
By adopting Fabric data ponds, organizations can:
* Accelerate development and analytics cycles
* Reduce unnecessary dependency on centralized platforms
* Move datasets between local and central environments safely
* Maintain strong governance and auditability
* Choose the appropriate scale for each workload

Fabric does not replace centralized platforms — it *complements* them by enabling modern, domain-oriented analytics workflows.
