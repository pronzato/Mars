{toc}
h1. End-to-End Use Case


h2. Scenario overview
This example shows how Fabric connects ingestion, storage, query, and entitlements into a single governed workflow using a realistic *orders history* use case.

The key point of this example is that *the same dataset identity, policies, and client code can be used with different architectures*, depending on how teams choose to deploy Fabric and how quickly upstream providers adopt it.

h3. Teams and responsibilities
* *OMS* (producer team)
** Owns the *orders_hist* dataset
** Publishes historical orders data (either via Fabric feeds or via an existing service)
** Defines entitlements and row-level policies for consumers
* *Desk1* (consumer team)
** Queries *orders_hist* using the Java Tabular API
* *Desk2* (consumer team)
** Queries *orders_hist* using the Python Tabular API
** Sees a filtered view of the data via row-level entitlements

All teams interact through Fabric’s canonical dataset identity: *orders_hist*.

h2. Step 1: OMS defines and ingests the dataset
The OMS team defines a governed dataset and begins ingesting orders history data.

h3. Define the dataset and schema (Feed API)
{code:language=java}
// OMS team — define a governed dataset with an inline schema
feeds.getOrCreate(
    FeedModel.builder()
        .withName("orders_hist")
        .withTarget(target -> target
            .withParquet(parquet -> parquet
                .withDirectory("s3://fabric-data/orders_hist")
                .withPartition(List.of("created_date"))
            )
        )
        .withSchema(
            FeedSchema.builder()
                .addField(field("order_id", FeedSchema.Type.STRING, false))
                .addField(field("desk", FeedSchema.Type.STRING, false))
                .addField(field("price", FeedSchema.Type.FLOAT64, false))
                .addField(field("created_date", FeedSchema.Type.DATE, false))
                .build()
        )
        .build()
);
{code}

h3. Ingest orders data
{code:language=java}
// OMS team — publish order records
feeds.ingest("orders_hist", List.of(
    Map.of(
        "order_id", "ORD-1001",
        "desk", "DESK1",
        "price", 125.50,
        "created_date", Date.valueOf(LocalDate.now())
    ),
    Map.of(
        "order_id", "ORD-1002",
        "desk", "DESK2",
        "price", 250.75,
        "created_date", Date.valueOf(LocalDate.now())
    )
));
{code}

Fabric validates schema, applies partitioning, and stores the data in governed Parquet.

{code}
orders_hist/
├── created_date=2026-01-30/
│   ├── part-00000-3f2a.parquet
│   ├── part-00001-3f2a.parquet
│   └── part-00002-3f2a.parquet
├── created_date=2026-01-29/
│   ├── part-00000-9c81.parquet
│   └── part-00001-9c81.parquet
├── created_date=2026-01-28/
│   ├── part-00000-a41e.parquet
│   ├── part-00001-a41e.parquet
│   ├── part-00002-a41e.parquet
│   └── part-00003-a41e.parquet
└── _metadata
{code}

h2. Step 2: OMS defines entitlements and row-level policy
OMS defines dataset access for both desks, and a row-level filter for Desk2.

h3. Dataset read access
{code:language=json}
{
  "resource": "dataset:orders_hist",
  "permissions": [
    { "role": "reader", "subject": "group:desk1" },
    { "role": "reader", "subject": "group:desk2" }
  ]
}
{code}

h3. Row-level entitlement for Desk2
Desk2 can only see orders where the *desk* field equals *DESK2*.

{code:language=json}
{
  "resource": "dataset:orders_hist",
  "subject": "group:desk2",
  "row_filter": "desk = 'DESK2'"
}
{code}

This policy is enforced automatically for every access path.

h2. Architecture #1 — Service-based (Arrow Flight SQL)
In this architecture, OMS (or a platform team) runs scalable Arrow Flight SQL services on their own infrastructure.

* Data is written locally or to shared storage.
* Arrow Flight SQL services query the data directly.
* Services scale horizontally on VMs or OpenShift.
* Clients route automatically to available service instances.
* Fabric enforces entitlements and OBO inside both client and service runtimes.

This model fits teams that already operate services and want full control over runtime placement.

h3. Desk1 queries from Java
{code:language=java}
// Desk1 — Java client
var session = Fabric.Session.builder()
    .environment("DEV")
    .group("desk1")
    .build();

var tabular = session.tabular();

var result = tabular.query("""
    SELECT order_id, desk, price
    FROM orders_hist
    ORDER BY created_date DESC
""");

System.out.println(result.getRowCount());
{code}

h3. Desk2 queries from Python (automatically filtered)
{code:language=python}
import fabric
import polars as pl

session = fabric.Session().environment("DEV").group("desk2").connect()
tabular = session.tabular()

result = tabular.query("""
    SELECT order_id, desk, price
    FROM orders_hist
    ORDER BY created_date DESC
""")

df = pl.from_arrow(result.to_arrow())
print(df)
{code}

Desk2 automatically receives only rows where *desk = 'DESK2'*.

h2. Architecture #2 — Service-free (direct storage access)
In this architecture, no Arrow Flight SQL services are required.

* OMS writes partitioned Parquet directly to S3.
* Clients query *orders_hist* directly through Fabric.
* Fabric handles metadata resolution, push-down, and entitlements inline.
* No long-running service layer is required for reads or writes.

This model is especially attractive for:
* Analytical workloads
* Batch or ad-hoc querying
* Teams that prefer minimal operational footprint

The *same dataset identity, entitlements, and client code* are used as in the service-based architecture.

h3. Desk clients (Java and Python)
The client code remains unchanged from Architecture #1.

Fabric resolves the dataset location and enforces policies inline, allowing clients to query directly against storage without a service proxy.

h2. Architecture #3 — Adapter-based (existing OMS gRPC service)
In this architecture, OMS has not adopted Fabric yet and continues to expose orders history through an existing gRPC service.

Fabric still enables a unified client experience by using a Tabular gRPC adapter that can be embedded in the client runtime:

* OMS runs its existing gRPC service unchanged.
* Fabric clients continue to use the Tabular API.
* The Tabular gRPC adapter translates Tabular requests into OMS gRPC calls.
* Responses are mapped into a tabular/columnar form (e.g. Arrow) for efficient client consumption.
* Entitlements and row-level policies are still applied consistently at the Fabric boundary.

This provides immediate value to clients without forcing OMS to migrate upfront — while naturally creating demand for a future native Fabric service when the team is ready.

h3. Desk2 queries via the gRPC adapter (Python)
{code:language=python}
import fabric
import polars as pl

session = fabric.Session().environment("DEV").group("desk2").connect()

# Tabular API remains the same; routing points to a gRPC adapter behind the scenes
tabular = session.tabular()

result = tabular.query("""
    SELECT order_id, desk, price
    FROM orders_hist
    ORDER BY created_date DESC
""")

df = pl.from_arrow(result.to_arrow())
print(df)
{code}

From the client perspective:
* The API is unchanged.
* The dataset identity is unchanged.
* The row-level policy for Desk2 is still enforced.

Only the backend integration path differs.

h2. What this demonstrates
This single use case demonstrates Fabric’s core differentiators:

* The same dataset identity and policies support multiple deployment architectures.
* Fabric is an embedded runtime, not a central data service.
* Service-based, service-free, and adapter-based models can coexist.
* Clients use one API regardless of architecture.
* Entitlements and row-level policies are enforced consistently across access paths.
* Teams remain decoupled:
** producers publish without coordinating with consumers
** consumers query without knowing storage or service topology
** governance is defined once and enforced everywhere
