Fabric API – S3 Integration

The Fabric API now supports seamless integration with Amazon S3 as a storage backend. This allows clients to switch effortlessly between local file storage and remote S3 storage without changing their applications or workflows. By using Fabric as the abstraction layer, teams can standardize how they write, query, and manage data regardless of where it lives.

With this integration:

Unified Storage Access: Write data locally for development or testing, then move to S3 for production-scale workloads with no code changes.

Pipeline-Driven Ingestion: Use the Fabric Pipeline API to ingest any source data into S3 as a partitioned set of Parquet files. This ensures datasets are schema-aligned, efficiently stored, and immediately queryable with SQL.

Self-Service Data Lake Complement: Fabric on S3 provides a lightweight, fully self-service complement to traditional data lake architectures. It enables teams to ingest, manage, and query datasets independently, while still integrating with broader enterprise data strategies.

Remote Querying: Use Fabric with DuckDB SQL to query Parquet files stored on S3 as if they were traditional relational tables. This enables powerful analytics and exploration directly on S3 data without requiring pre-ingestion into a database.

Schema-Aware Management: Manage Parquet files on S3 with the same schema-driven approach Fabric provides for local data, ensuring compatibility, discoverability, and consistency across environments.

Operational Flexibility: Applications can transparently choose storage location (local vs. S3) at runtime, making it easier to support hybrid workflows and optimize for performance, cost, or compliance.

Data Ingestion with the Pipeline API

Clients can also leverage the Fabric Pipeline API to ingest any source data directly into S3 as a partitioned set of Parquet files. This provides:

Automated Partitioning: Fabric automatically partitions data (by time, symbol, region, or other schema-defined fields), making large datasets efficient to store and query.

Schema Enforcement: Input data is validated and transformed into Parquet according to the target schema, ensuring downstream compatibility.

Unified Ingest Layer: Whether data originates from CSV, JSON, Arrow buffers, or other formats, the Pipeline API provides a consistent entry point into S3.

Optimized for Querying: Partitioned Parquet files are directly queryable via DuckDB SQL, enabling scalable analytics on large datasets stored in S3 without additional ETL steps.

Self-Service Flexibility: This capability gives teams an easy way to set up their own pipelines and explore data without heavy infrastructure. At the same time, it complements enterprise data lakes by offering a faster, more agile path for development, prototyping, and team-specific workloads.



s3://analytics-bucket/quotes/
  dt=2025-09-15/symbol=AAPL/part-0001.parquet
  dt=2025-09-15/symbol=MSFT/part-0002.parquet
  dt=2025-09-16/symbol=AAPL/part-0001.parquet
  dt=2025-09-16/symbol=MSFT/part-0001.parquet
Fabric writes Hive-style partitions (e.g., dt=.../symbol=.../) so DuckDB can prune partitions automatically from WHERE filters.


Iceberg on S3 (No Spark Required)
We’re adding Fabric API support for Apache Iceberg tables on S3 without mandating a Spark runtime. The goal is to keep the same self-service workflow (ingest → manage → query) while gaining table-format capabilities:

Table Management via Fabric: create tables, evolve schemas, manage partition specs, and handle snapshot/rollback operations through simple Fabric APIs.

ACID & Time-Travel: leverage Iceberg’s metadata and snapshots for safe concurrent writes and historical queries.

Performance Hygiene: compaction, manifest rewrites, and retention policies exposed as Fabric maintenance operations.

Open Catalogs: support for REST/Hive catalogs so teams can interoperate with other engines if needed—while still using Fabric + DuckDB SQL for day-to-day querying.

Same Developer Experience: keep using partitioned Parquet under the hood; Fabric promotes compatible datasets to Iceberg tables when you need table semantics.
